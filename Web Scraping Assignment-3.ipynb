{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49c52d75-fc9c-4aaa-a246-85af08a645c2",
   "metadata": {},
   "source": [
    "### 1. Write a python program which searches all the product under a particular product from www.amazon.in. The product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search for guitars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bb99664-3eb0-4acf-a2bf-bdfd06c9e39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the product you want to search for on Amazon:  guitar\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructed URL: https://www.amazon.in/s/ref=nb_sb_noss?url=search-alias%3Daps&field-keywords=guitar\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "product_to_search = input(\"Enter the product you want to search for on Amazon: \")\n",
    "\n",
    "search_url = \"https://www.amazon.in/s/ref=nb_sb_noss?url=search-alias%3Daps&field-keywords=\" + product_to_search\n",
    "\n",
    "print(\"Constructed URL:\", search_url)\n",
    "time.sleep(5) \n",
    "\n",
    "response = requests.get(search_url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    product_containers = soup.find_all('div', class_='s-result-item')\n",
    "\n",
    "    if product_containers:\n",
    "        for container in product_containers:\n",
    "            product_title = container.find('span', class_='a-size-medium a-color-base a-text-normal').text.strip()\n",
    "            product_price_element = container.find('span', class_='a-offscreen')\n",
    "            product_price = product_price_element.text.strip() if product_price_element else \"-\"\n",
    "            print(\"Product:\", product_title)\n",
    "            print(\"Price:\", product_price)\n",
    "            print(\"-----------------------------------------\")\n",
    "    else:\n",
    "        print(\"No products found for '{}'\".format(product_to_search))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970a3d8d-e861-4d56-9c71-df64107df59d",
   "metadata": {},
   "source": [
    "### 2. In the above question, now scrape the following details of each product listed in first 3 pages of your search results and save it in a data frame and csv. In case if any product has less than 3 pages in search results then scrape all the products available under that product name. Details to be scraped are: \"Brand Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\" and “Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62f69beb-0816-486e-8d55-d051a555d675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the product you want to search for on Amazon:  shoes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructed URL: https://www.amazon.in/s?k=shoes\n",
      "Current Page URL: https://www.amazon.in/s?k=shoes&page=1\n",
      "Failed to retrieve search results for page 1\n",
      "Current Page URL: https://www.amazon.in/s?k=shoes&page=2\n",
      "Failed to retrieve search results for page 2\n",
      "Current Page URL: https://www.amazon.in/s?k=shoes&page=3\n",
      "Failed to retrieve search results for page 3\n",
      "Scraping and saving of data completed.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "product_to_search = input(\"Enter the product you want to search for on Amazon: \")\n",
    "\n",
    "search_url = \"https://www.amazon.in/s?k=\" + product_to_search\n",
    "print(\"Constructed URL:\", search_url)\n",
    "\n",
    "brand_names = []\n",
    "product_names = []\n",
    "product_prices = []\n",
    "return_exchanges = []\n",
    "expected_deliveries = []\n",
    "availabilities = []\n",
    "product_urls = []\n",
    "\n",
    "for page_number in range(1, 4):\n",
    "    current_page_url = search_url + \"&page=\" + str(page_number)\n",
    "    print(\"Current Page URL:\", current_page_url)\n",
    "    time.sleep(5)  \n",
    "\n",
    "    response = requests.get(current_page_url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        product_containers = soup.find_all('div', class_='s-result-item')\n",
    "\n",
    "        if product_containers:\n",
    "            for container in product_containers:\n",
    "                product_link = container.find('a', class_='a-link-normal')\n",
    "                if product_link:\n",
    "                    product_url = product_link.get('href')\n",
    "                    if product_url and not product_url.startswith('http'):\n",
    "                        product_url = \"https://www.amazon.in\" + product_url\n",
    "\n",
    "                    product_response = requests.get(product_url)\n",
    "\n",
    "                    if product_response.status_code == 200:\n",
    "                        product_soup = BeautifulSoup(product_response.content, 'html.parser')\n",
    "                        brand_name = product_soup.find('a', class_='a-link-normal').text.strip()\n",
    "                        product_name_element = product_soup.find('span', id='productTitle')\n",
    "                        product_name = product_name_element.text.strip() if product_name_element else \"-\"\n",
    "                        product_price_element = product_soup.find('span', id='priceblock_ourprice')\n",
    "                        product_price = product_price_element.text.strip() if product_price_element else \"-\"\n",
    "                        return_exchange = product_soup.find('div', class_='a-section a-spacing-mini').text.strip().replace('\\n', '').replace('  ', '')\n",
    "                        expected_delivery_tag = product_soup.find('div', id='ddmDeliveryMessage')\n",
    "                        expected_delivery = expected_delivery_tag.text.strip() if expected_delivery_tag else \"-\"\n",
    "                        availability_tag = product_soup.find('div', id='availability')\n",
    "                        availability = availability_tag.text.strip() if availability_tag else \"-\"\n",
    "\n",
    "                        brand_names.append(brand_name)\n",
    "                        product_names.append(product_name)\n",
    "                        product_prices.append(product_price)\n",
    "                        return_exchanges.append(return_exchange)\n",
    "                        expected_deliveries.append(expected_delivery)\n",
    "                        availabilities.append(availability)\n",
    "                        product_urls.append(product_url)\n",
    "                    else:\n",
    "                        print(\"Failed to retrieve product details for:\", product_url)\n",
    "                else:\n",
    "                    print(\"Product link not found on page {} for '{}'\".format(page_number, product_to_search))\n",
    "        else:\n",
    "            print(\"No products found on page {} for '{}'\".format(page_number, product_to_search))\n",
    "    else:\n",
    "        print(\"Failed to retrieve search results for page {}\".format(page_number))\n",
    "\n",
    "brand_names = [name if name else \"-\" for name in brand_names]\n",
    "product_names = [name if name else \"-\" for name in product_names]\n",
    "product_prices = [price if price else \"-\" for price in product_prices]\n",
    "return_exchanges = [exchange if exchange else \"-\" for exchange in return_exchanges]\n",
    "expected_deliveries = [delivery if delivery else \"-\" for delivery in expected_deliveries]\n",
    "availabilities = [availability if availability else \"-\" for availability in availabilities]\n",
    "product_urls = [url if url else \"-\" for url in product_urls]\n",
    "\n",
    "data = {\n",
    "    'Brand Name': brand_names,\n",
    "    'Name of the Product': product_names,\n",
    "    'Price': product_prices,\n",
    "    'Return/Exchange': return_exchanges,\n",
    "    'Expected Delivery': expected_deliveries,\n",
    "    'Availability': availabilities,\n",
    "    'Product URL': product_urls\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.to_csv('amazon_products.csv', index=False)\n",
    "\n",
    "print(\"Scraping and saving of data completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e64e02b-03d9-4ddd-95f6-11244ef4a903",
   "metadata": {},
   "source": [
    "### 4. Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com and scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”, “Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the details is missing then replace it by “- “. Save your results in a dataframe and CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36e2378b-6aaa-4e16-9f96-e2182d57ff6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the smartphone you want to search for on Flipkart:  watch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data scraped and saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_flipkart_smartphones(search_query):\n",
    "    # Construct the search URL\n",
    "    search_url = f\"https://www.flipkart.com/search?q={search_query}&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off\"\n",
    "\n",
    "    response = requests.get(search_url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        product_containers = soup.find_all('div', class_='_1AtVbE')\n",
    "\n",
    "        data = {\n",
    "            'Brand Name': [],\n",
    "            'Smartphone Name': [],\n",
    "            'Colour': [],\n",
    "            'RAM': [],\n",
    "            'Storage(ROM)': [],\n",
    "            'Primary Camera': [],\n",
    "            'Secondary Camera': [],\n",
    "            'Display Size': [],\n",
    "            'Battery Capacity': [],\n",
    "            'Price': [],\n",
    "            'Product URL': []\n",
    "        }\n",
    "\n",
    "        for container in product_containers:\n",
    "            brand_name_elem = container.find('div', class_='_4rR01T')\n",
    "            brand_name = brand_name_elem.text.strip() if brand_name_elem else \"-\"\n",
    "            data['Brand Name'].append(brand_name)\n",
    "\n",
    "       \n",
    "            smartphone_name_elem = container.find('a', class_='IRpwTa')\n",
    "            smartphone_name = smartphone_name_elem.text.strip() if smartphone_name_elem else \"-\"\n",
    "            data['Smartphone Name'].append(smartphone_name)\n",
    "\n",
    "        \n",
    "            colour_elem = container.find('a', class_='IRpwTa')\n",
    "            colour = colour_elem.text.strip() if colour_elem else \"-\"\n",
    "            data['Colour'].append(colour)\n",
    "            ram_elem = container.find(string='RAM')\n",
    "            ram = ram_elem.find_next('li').text.strip() if ram_elem else \"-\"\n",
    "            data['RAM'].append(ram)\n",
    "            storage_elem = container.find(string='Storage')\n",
    "            storage = storage_elem.find_next('li').text.strip() if storage_elem else \"-\"\n",
    "            data['Storage(ROM)'].append(storage)\n",
    "            primary_camera_elem = container.find(string='Primary Camera')\n",
    "            primary_camera = primary_camera_elem.find_next('li').text.strip() if primary_camera_elem else \"-\"\n",
    "            data['Primary Camera'].append(primary_camera)\n",
    "            secondary_camera_elem = container.find(string='Secondary Camera')\n",
    "            secondary_camera = secondary_camera_elem.find_next('li').text.strip() if secondary_camera_elem else \"-\"\n",
    "            data['Secondary Camera'].append(secondary_camera)\n",
    "            display_size_elem = container.find(string='Display Size')\n",
    "            display_size = display_size_elem.find_next('li').text.strip() if display_size_elem else \"-\"\n",
    "            data['Display Size'].append(display_size)\n",
    "            battery_capacity_elem = container.find(string='Battery Capacity')\n",
    "            battery_capacity = battery_capacity_elem.find_next('li').text.strip() if battery_capacity_elem else \"-\"\n",
    "            data['Battery Capacity'].append(battery_capacity)\n",
    "            price_elem = container.find('div', class_='_30jeq3 _1_WHN1')\n",
    "            price = price_elem.text.strip() if price_elem else \"-\"\n",
    "            data['Price'].append(price)\n",
    "            product_url_elem = container.find('a', class_='IRpwTa')\n",
    "            product_url = 'https://www.flipkart.com' + product_url_elem['href'] if product_url_elem else \"-\"\n",
    "            data['Product URL'].append(product_url)\n",
    "        df = pd.DataFrame(data)\n",
    "        df.to_csv('flipkart_smartphones.csv', index=False)\n",
    "\n",
    "        print(\"Data scraped and saved successfully.\")\n",
    "\n",
    "    else:\n",
    "        print(\"Failed to retrieve search results.\")\n",
    "\n",
    "search_query = input(\"Enter the smartphone you want to search for on Flipkart: \")\n",
    "scrape_flipkart_smartphones(search_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a9162d-1382-4641-aea8-cd8f003cbcd3",
   "metadata": {},
   "source": [
    "### 6. Write a program to scrap all the available details of best gaming laptops from digit.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d43ca42-588b-4ac9-8f03-a1be44ee44dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data scraped and saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_gaming_laptops():\n",
    "    url = 'https://www.digit.in/top-products/best-gaming-laptops-40.html'\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        laptops = soup.find_all('div', class_='TopNumbeHeading sticky-footer')\n",
    "        data = {\n",
    "            'Name': [],\n",
    "            'Price': [],\n",
    "            'Processor': [],\n",
    "            'RAM': [],\n",
    "            'OS': [],\n",
    "            'Display': [],\n",
    "            'Resolution': [],\n",
    "            'Battery': []\n",
    "        }\n",
    "\n",
    "        for laptop in laptops:\n",
    "            data['Name'].append(laptop.h3.a.text.strip())\n",
    "            specs = laptop.find_next('div', class_='Section-center')\n",
    "            data['Price'].append(specs.find('div', class_='smprice').text.strip())\n",
    "            data['Processor'].append(specs.find_all('div', class_='value')[0].text.strip())\n",
    "            data['RAM'].append(specs.find_all('div', class_='value')[1].text.strip())\n",
    "            data['OS'].append(specs.find_all('div', class_='value')[2].text.strip())\n",
    "            data['Display'].append(specs.find_all('div', class_='value')[3].text.strip())\n",
    "            data['Resolution'].append(specs.find_all('div', class_='value')[4].text.strip())\n",
    "            data['Battery'].append(specs.find_all('div', class_='value')[5].text.strip())\n",
    "\n",
    "        df = pd.DataFrame(data)\n",
    "        df.to_csv('gaming_laptops.csv', index=False)\n",
    "        print(\"Data scraped and saved successfully.\")\n",
    "    else:\n",
    "        print(\"Failed to retrieve data from digit.in\")\n",
    "\n",
    "scrape_gaming_laptops()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb1c88f-8f90-4c10-acd6-3c156ab9ce74",
   "metadata": {},
   "source": [
    "### 7. Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped: “Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d4a38ef-8a56-476b-97ed-eaf113ef432f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data scraped and saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_billionaires():\n",
    "    url = \"https://www.forbes.com/billionaires/\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        billionaires = soup.find_all('div', class_='personName')\n",
    "\n",
    "        data = {\n",
    "            'Rank': [],\n",
    "            'Name': [],\n",
    "            'Net Worth': [],\n",
    "            'Age': [],\n",
    "            'Citizenship': [],\n",
    "            'Source': [],\n",
    "            'Industry': []\n",
    "        }\n",
    "\n",
    "        for i, billionaire in enumerate(billionaires, start=1):\n",
    "            data['Rank'].append(i)\n",
    "            data['Name'].append(billionaire.text.strip())\n",
    "            details = billionaire.find_next('div', class_='netWorth').text.strip().split('\\n')\n",
    "            data['Net Worth'].append(details[0])\n",
    "            data['Age'].append(details[1])\n",
    "            data['Citizenship'].append(details[2])\n",
    "            data['Source'].append(details[3])\n",
    "            data['Industry'].append(details[4])\n",
    "\n",
    "        df = pd.DataFrame(data)\n",
    "        df.to_csv('billionaires.csv', index=False)\n",
    "        print(\"Data scraped and saved successfully.\")\n",
    "    else:\n",
    "        print(\"Failed to retrieve data from Forbes.\")\n",
    "\n",
    "scrape_billionaires()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e522ee54-fee6-4aee-aff4-b5399b76cf33",
   "metadata": {},
   "source": [
    "### 9. Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in “London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall reviews, privates from price, dorms from price, facilities and property description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a62cf8f-e4dd-4841-8523-467668743e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_hostels_in_london():\n",
    "    url = \"https://www.hostelworld.com/s?q=London,%20England&country=England&city=London&type=city&id=3&from=2022-03-08&to=2022-03-11&guests=1&page=1\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        hostel_listings = soup.find_all('div', class_='fabresult')\n",
    "\n",
    "        for hostel in hostel_listings:\n",
    "            name = hostel.find('h2', class_='title-2').text.strip()\n",
    "            distance = hostel.find('span', class_='label').text.strip()\n",
    "            ratings = hostel.find('div', class_='score').text.strip()\n",
    "            total_reviews = hostel.find('div', class_='reviews').text.strip()\n",
    "            overall_reviews = hostel.find('div', class_='rating').text.strip()\n",
    "            privates_price = hostel.find('div', class_='price').text.strip()\n",
    "            dorms_price = hostel.find('div', class_='dorms').text.strip()\n",
    "            facilities = [item.text.strip() for item in hostel.find_all('span', class_='rating-factors')]\n",
    "            description = hostel.find('div', class_='desc').text.strip()\n",
    "\n",
    "            print(\"Name:\", name)\n",
    "            print(\"Distance from city centre:\", distance)\n",
    "            print(\"Ratings:\", ratings)\n",
    "            print(\"Total reviews:\", total_reviews)\n",
    "            print(\"Overall reviews:\", overall_reviews)\n",
    "            print(\"Privates from price:\", privates_price)\n",
    "            print(\"Dorms from price:\", dorms_price)\n",
    "            print(\"Facilities:\", facilities)\n",
    "            print(\"Description:\", description)\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "    else:\n",
    "        print(\"Failed to retrieve data from Hostelworld.\")\n",
    "\n",
    "scrape_hostels_in_london()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
