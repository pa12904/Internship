{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10c14edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully retrieved the webpage.\n",
      "Successfully performed the job search.\n",
      "Scraped Data:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#Q1\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.shine.com/\"\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Successfully retrieved the webpage.\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
    "    exit()\n",
    "\n",
    "job_title = \"Data Analyst\"\n",
    "location = \"Bangalore\"\n",
    "\n",
    "search_payload = {\n",
    "    \"query\": job_title,\n",
    "    \"location\": location,\n",
    "}\n",
    "\n",
    "search_url = \"https://www.shine.com/job-search/datascienceml\"\n",
    "search_response = requests.get(search_url, params=search_payload)\n",
    "\n",
    "if search_response.status_code == 200:\n",
    "    print(\"Successfully performed the job search.\")\n",
    "else:\n",
    "    print(f\"Failed to perform the job search. Status code: {search_response.status_code}\")\n",
    "    exit()\n",
    "\n",
    "soup = BeautifulSoup(search_response.content, \"html.parser\")\n",
    "\n",
    "job_data = []\n",
    "for job_card in soup.find_all(\"div\", class_=\"w-100 mb-10 job-box js-job-box\"):\n",
    "    title = job_card.find(\"div\", class_=\"job-card-title\").text.strip()\n",
    "    location = job_card.find(\"span\", class_=\"job-location\").text.strip()\n",
    "    company_name = job_card.find(\"a\", class_=\"job-title-text\").text.strip()\n",
    "    experience_required = job_card.find(\"span\", class_=\"exp_span\").text.strip()\n",
    "\n",
    "    job_data.append({\n",
    "        \"Job Title\": title,\n",
    "        \"Job Location\": location,\n",
    "        \"Company Name\": company_name,\n",
    "        \"Experience Required\": experience_required,\n",
    "    })\n",
    "\n",
    "    if len(job_data) >= 10:\n",
    "        break\n",
    "\n",
    "df = pd.DataFrame(job_data)\n",
    "\n",
    "print(\"Scraped Data:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cfc95b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43f12b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully retrieved the webpage.\n",
      "Successfully performed the job search.\n",
      "Scraped Data:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#Q2\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.shine.com/\"\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Successfully retrieved the webpage.\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
    "    exit()\n",
    "\n",
    "job_title = \"Data Scientist\"\n",
    "location = \"Bangalore\"\n",
    "\n",
    "search_payload = {\n",
    "    \"query\": job_title,\n",
    "    \"location\": location,\n",
    "}\n",
    "\n",
    "search_url = \"https://www.shine.com/job-search/datascienceml\"\n",
    "search_response = requests.get(search_url, params=search_payload)\n",
    "\n",
    "if search_response.status_code == 200:\n",
    "    print(\"Successfully performed the job search.\")\n",
    "else:\n",
    "    print(f\"Failed to perform the job search. Status code: {search_response.status_code}\")\n",
    "    exit()\n",
    "\n",
    "soup = BeautifulSoup(search_response.content, \"html.parser\")\n",
    "\n",
    "job_data = []\n",
    "for job_card in soup.find_all(\"div\", class_=\"w-100 mb-10 job-box js-job-box\"):\n",
    "    title = job_card.find(\"div\", class_=\"job-card-title\").text.strip()\n",
    "    location = job_card.find(\"span\", class_=\"job-location\").text.strip()\n",
    "    company_name = job_card.find(\"a\", class_=\"job-title-text\").text.strip()\n",
    "\n",
    "    job_data.append({\n",
    "        \"Job Title\": title,\n",
    "        \"Job Location\": location,\n",
    "        \"Company Name\": company_name,\n",
    "    })\n",
    "\n",
    "    if len(job_data) >= 10:\n",
    "        break\n",
    "\n",
    "df = pd.DataFrame(job_data)\n",
    "\n",
    "print(\"Scraped Data:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97d72fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160eed0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_flipkart_page(url):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        product_cards = soup.find_all('div', class_='_1AtVbE')\n",
    "\n",
    "        for card in product_cards:\n",
    "            brand = card.find('div', class_='_2WkVRV').text.strip()\n",
    "            description = card.find('a', class_='IRpwTa').text.strip()\n",
    "            price = card.find('div', class_='_30jeq3').text.strip()\n",
    "\n",
    "            data.append({\n",
    "                'Brand': brand,\n",
    "                'Product Description': description,\n",
    "                'Price': price\n",
    "            })\n",
    "\n",
    "base_url = 'https://www.flipkart.com/'\n",
    "search_query = 'sunglasses'\n",
    "data = []\n",
    "\n",
    "for page in range(1, 11):\n",
    "    search_url = f'{base_url}search?q={search_query}&page={page}'\n",
    "    scrape_flipkart_page(search_url)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Scraped Data:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d101df2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7231265",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape reviews data from Flipkart\n",
    "def scrape_flipkart_reviews(url, max_reviews=100):\n",
    "    reviews_data = []\n",
    "\n",
    "    for page_num in range(1, (max_reviews // 10) + 2):\n",
    "        page_url = f\"{url}&page={page_num}\"\n",
    "\n",
    "        response = requests.get(page_url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            review_containers = soup.find_all('div', class_='_1AtVbE')\n",
    "\n",
    "            for container in review_containers:\n",
    "                rating = container.find('div', class_='hGSR34').text.strip()\n",
    "                summary = container.find('p', class_='_2xg6Ul').text.strip()\n",
    "                full_review = container.find('div', class_='qwjRop').text.strip()\n",
    "\n",
    "                reviews_data.append({\n",
    "                    'Rating': rating,\n",
    "                    'Review Summary': summary,\n",
    "                    'Full Review': full_review\n",
    "                })\n",
    "\n",
    "                if len(reviews_data) >= max_reviews:\n",
    "                    break\n",
    "\n",
    "        else:\n",
    "            print(f\"Failed to fetch data for page {page_num}. Status code: {response.status_code}\")\n",
    "            break\n",
    "\n",
    "    return reviews_data\n",
    "\n",
    "# Main scraping process\n",
    "product_reviews_url = \"https://www.flipkart.com/apple-iphone-11-black-64-gb/product-reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART\"\n",
    "reviews_data = scrape_flipkart_reviews(product_reviews_url, max_reviews=100)\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "df = pd.DataFrame(reviews_data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Scraped Reviews Data:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483071a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc45295a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_flipkart_sneakers(max_sneakers=100):\n",
    "    base_url = 'https://www.flipkart.com'\n",
    "    search_query = 'sneakers'\n",
    "    sneakers_data = []\n",
    "\n",
    "    for page_num in range(1, (max_sneakers // 40) + 2):\n",
    "        search_url = f'{base_url}/search?q={search_query}&page={page_num}'\n",
    "        response = requests.get(search_url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            product_cards = soup.find_all('div', class_='_1AtVbE')\n",
    "\n",
    "            for card in product_cards:\n",
    "                brand = card.find('div', class_='_2WkVRV').text.strip()\n",
    "                description = card.find('a', class_='IRpwTa').text.strip()\n",
    "                price = card.find('div', class_='_30jeq3').text.strip()\n",
    "\n",
    "                sneakers_data.append({\n",
    "                    'Brand': brand,\n",
    "                    'Product Description': description,\n",
    "                    'Price': price\n",
    "                })\n",
    "\n",
    "                if len(sneakers_data) >= max_sneakers:\n",
    "                    break\n",
    "\n",
    "        else:\n",
    "            print(f\"Failed to fetch data for page {page_num}. Status code: {response.status_code}\")\n",
    "            break\n",
    "\n",
    "    return sneakers_data\n",
    "\n",
    "sneakers_data = scrape_flipkart_sneakers(max_sneakers=100)\n",
    "\n",
    "df = pd.DataFrame(sneakers_data)\n",
    "\n",
    "print(\"Scraped Sneakers Data:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de621dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac39d8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q9\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.jagranjosh.com/\"\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code != 200:\n",
    "    print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
    "    exit()\n",
    "\n",
    "prime_ministers_url = \"https://www.jagranjosh.com/general-knowledge/list-of-prime-ministers-of-india-till-now-1540459906-1\"\n",
    "response_prime_ministers = requests.get(prime_ministers_url)\n",
    "\n",
    "if response_prime_ministers.status_code != 200:\n",
    "    print(f\"Failed to retrieve the Prime Ministers page. Status code: {response_prime_ministers.status_code}\")\n",
    "    exit()\n",
    "\n",
    "soup = BeautifulSoup(response_prime_ministers.content, 'html.parser')\n",
    "table = soup.find('table', class_='styled-table')\n",
    "\n",
    "if not table:\n",
    "    print(\"Table not found on the page.\")\n",
    "    exit()\n",
    "\n",
    "data = []\n",
    "for row in table.find_all('tr')[1:]:  # Skip the header row\n",
    "    columns = row.find_all('td')\n",
    "    name = columns[0].text.strip()\n",
    "    born_dead = columns[1].text.strip()\n",
    "    term_of_office = columns[2].text.strip()\n",
    "    remarks = columns[3].text.strip()\n",
    "\n",
    "    data.append({\n",
    "        'Name': name,\n",
    "        'Born-Dead': born_dead,\n",
    "        'Term of Office': term_of_office,\n",
    "        'Remarks': remarks\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Scraped Data:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e44849",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4147bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q10\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.motor1.com/\"\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code != 200:\n",
    "    print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
    "    exit()\n",
    "\n",
    "search_url = \"https://www.motor1.com/search?q=50+most+expensive+cars\"\n",
    "response_search = requests.get(search_url)\n",
    "\n",
    "if response_search.status_code != 200:\n",
    "    print(f\"Failed to perform the search. Status code: {response_search.status_code}\")\n",
    "    exit()\n",
    "\n",
    "soup_search = BeautifulSoup(response_search.content, 'html.parser')\n",
    "result_link = soup_search.find('a', text='50 most expensive cars in the world')\n",
    "\n",
    "if not result_link:\n",
    "    print(\"Link not found for '50 most expensive cars in the world'.\")\n",
    "    exit()\n",
    "\n",
    "result_url = result_link['href']\n",
    "response_result = requests.get(result_url)\n",
    "\n",
    "if response_result.status_code != 200:\n",
    "    print(f\"Failed to retrieve the result page. Status code: {response_result.status_code}\")\n",
    "    exit()\n",
    "\n",
    "soup_result = BeautifulSoup(response_result.content, 'html.parser')\n",
    "car_data = []\n",
    "\n",
    "for car_card in soup_result.find_all('div', class_='p__wrp p__wrp-single-list'):\n",
    "    car_name = car_card.find('h2').text.strip()\n",
    "    car_price = car_card.find('div', class_='p__prc').text.strip()\n",
    "\n",
    "    car_data.append({\n",
    "        'Car Name': car_name,\n",
    "        'Price': car_price\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(car_data)\n",
    "\n",
    "print(\"Scraped Data:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7588cdab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49866a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28906a75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c892b0f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fae49a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb61cb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc53ba6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
